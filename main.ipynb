{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython import get_ipython\n",
    "from scipy import spatial\n",
    "import heapq\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Get the list of movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The follow code take all the link which are in files movies 1, 2 ,3 and put them in list \"link\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "movies1=open(r'C:\\Users\\simo2\\OneDrive\\Desktop\\arishomework3\\filmproject.html', \"r\")\n",
    "soup_movies1 = BeautifulSoup(movies1, 'lxml')\n",
    "\n",
    "movies2=open(r'C:\\Users\\simo2\\OneDrive\\Desktop\\arishomework3\\filmproject2.html', \"r\")\n",
    "soup_movies2 = BeautifulSoup(movies2, 'lxml')\n",
    "\n",
    "movies3=open(r'C:\\Users\\simo2\\OneDrive\\Desktop\\arishomework3\\filmproject3.html', \"r\")\n",
    "soup_movies3 = BeautifulSoup(movies3, 'lxml')\n",
    "\n",
    "for tag in soup_movies1.find_all('a', href=True):\n",
    "    links.append(tag['href'])\n",
    "\n",
    "for tag in soup_movies2.find_all('a', href=True):\n",
    "    links.append(tag['href'])\n",
    "    \n",
    "for tag in soup_movies3.find_all('a', href=True):\n",
    "    links.append(tag['href'])\n",
    "print(links)    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Crawl Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point we have a for loop through the list \"links\" and we  obtain the different lists of urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for l in links:\n",
    "    try:\n",
    "        response = requests.get(l)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        time.sleep(2)\n",
    "        path_directory=r'C:\\Users\\HP\\Desktop\\file hw3\\article_'+ str(i) +'.html'\n",
    "        \n",
    "        with open(path_directory, \"w\", encoding='utf-8') as file:\n",
    "            file.write(str(soup))\n",
    "            file.close()\n",
    "        i+=1    \n",
    "    except:\n",
    "        time.sleep(1210) \n",
    "        response = requests.get(l)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        time.sleep(2)\n",
    "        path_directory=r'C:\\Users\\HP\\Desktop\\file hw3\\article_'+ str(i) +'.html'\n",
    "        \n",
    "        with open(path_directory, \"w\", encoding='utf-8') as file:\n",
    "            file.write(str(soup))\n",
    "            file.close()\n",
    "        i+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each link we take title, and the first 2 sections and write a tsv file for each wiki page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = 0\n",
    "while time <=30000:\n",
    "    try:\n",
    "        with open(r'C:\\Users\\HP\\Desktop\\file hw3\\article_'+ str(time) +'.html', \"r\", encoding='utf-8') as file:\n",
    "            articolo=file.read()\n",
    "        soup = BeautifulSoup(articolo,'html.parser')\n",
    "        \n",
    "        \n",
    "        title=soup.find_all('title')        \n",
    "        title = title[0].get_text()\n",
    "        title = title.split('-')\n",
    "        title = str(title[0])\n",
    "        mydiv = soup.find_all(\"div\", class_=\"mw-parser-output\")\n",
    "        mydiv=mydiv[0] \n",
    "        par_and_head=mydiv.find_all([\"p\", \"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "        headings_list = ['h2','h3','h4','h5','h6']\n",
    "        intro = ''\n",
    "        plot = ''\n",
    "        for i in range(len(par_and_head)):\n",
    "            if par_and_head[i].name=='p':\n",
    "                intro += par_and_head[i].get_text()\n",
    "            if par_and_head[i].name in headings_list and intro != '':\n",
    "                variabile = i\n",
    "                break   \n",
    "        while variabile < len(par_and_head):\n",
    "            if par_and_head[variabile].name == 'p':\n",
    "                plot += par_and_head[variabile].get_text()\n",
    "            elif par_and_head[variabile].name in headings_list and plot != '':\n",
    "                break\n",
    "            variabile+=1\n",
    "        \n",
    "           \n",
    "        table=soup.find('table',{'class':'infobox vevent'})\n",
    "        article_table_dataframe = pd.read_html(str(table))\n",
    "        article_table_dataframe=article_table_dataframe[0]\n",
    "        article_table_dataframe.columns = ['Category','info']\n",
    "\n",
    "        for i in range(len(article_table_dataframe)):\n",
    "            for j in range(len(article_table_dataframe['info'][i])):\n",
    "                if j!= 0:\n",
    "                    if article_table_dataframe['info'][i][j].isupper() and article_table_dataframe['info'][i][j-1].islower():\n",
    "                       article_table_dataframe['info'][i]= article_table_dataframe['info'][i][:j]+' '+article_table_dataframe['info'][i][j:]\n",
    "        data_category = ['Title','Intro','Plot']\n",
    "        data_to_merge1 = pd.DataFrame(data_category)\n",
    "        data_tip = [title,intro,plot]\n",
    "        data_to_merge2 = pd.DataFrame(data_tip)\n",
    "        data_tip_merged=pd.merge(data_to_merge1, data_to_merge2, left_index=True, right_index=True) \n",
    "        data_tip_merged.columns = ['Category', 'info']\n",
    "        film_info = pd.concat([article_table_dataframe, data_tip_merged], ignore_index=True)\n",
    "        film_info=film_info.dropna(how='all')\n",
    "        film_info=film_info.reset_index(drop=True)\n",
    "        data_info_names=['Title','Intro','Plot', 'Directed by', 'Produced by', 'Written By', 'Starring', 'Music by', 'Release date', 'Running time', 'Country', 'Language', 'Budget']\n",
    "        dataframe_info_names=pd.DataFrame(data_info_names)\n",
    "        dataframe_info_names.columns=['Category']\n",
    "        final_film_dataframe=pd.merge(dataframe_info_names, film_info, on='Category',how='left')\n",
    "        with open(r'C:\\Users\\HP\\Desktop\\file hw3\\wiki\\article_'+ str(time)+'.tsv', 'wt',encoding='utf-8') as out_file:\n",
    "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "           \n",
    "            tsv_writer.writerow(final_film_dataframe['Category'])   \n",
    "            tsv_writer.writerow(final_film_dataframe['info'])\n",
    "        time+=1\n",
    "    except:\n",
    "        \n",
    "        \n",
    "        with open(r'C:\\Users\\HP\\Desktop\\file hw3\\article_'+ str(time) +'.html', \"r\", encoding='utf-8') as file:\n",
    "            articolo=file.read()\n",
    "        soup = BeautifulSoup(articolo,'html.parser')\n",
    "        title=soup.find_all('title')        \n",
    "        \n",
    "        title = title[0].get_text()\n",
    "        title = title.split('-')\n",
    "        title = str(title[0])\n",
    "        mydiv = soup.find_all(\"div\", class_=\"mw-parser-output\")\n",
    "        if len(mydiv)!=0:\n",
    "            mydiv=mydiv[0] \n",
    "            par_and_head=mydiv.find_all([\"p\", \"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "            headings_list = ['h2','h3','h4','h5','h6']\n",
    "            intro = ''\n",
    "            plot = ''\n",
    "            for i in range(len(par_and_head)):\n",
    "                if par_and_head[i].name=='p':\n",
    "                    intro += par_and_head[i].get_text()\n",
    "                if par_and_head[i].name in headings_list and intro != '':\n",
    "                    variabile = i\n",
    "                    break   \n",
    "            while variabile < len(par_and_head):\n",
    "                if par_and_head[variabile].name == 'p':\n",
    "                    plot += par_and_head[variabile].get_text()\n",
    "                elif par_and_head[variabile].name in headings_list and plot != '':\n",
    "                    break\n",
    "                variabile+=1\n",
    "            data_category = ['Title','Intro','Plot']\n",
    "            data_to_merge1 = pd.DataFrame(data_category)\n",
    "            data_tip = [title,intro,plot]\n",
    "            data_to_merge2 = pd.DataFrame(data_tip)\n",
    "            data_tip_merged=pd.merge(data_to_merge1, data_to_merge2, left_index=True, right_index=True) \n",
    "            data_tip_merged.columns = ['Category', 'info']\n",
    "            data_tip_merged=data_tip_merged.dropna(how='all')\n",
    "            data_tip_merged=data_tip_merged.reset_index(drop=True)\n",
    "            with open(r'C:\\Users\\HP\\Desktop\\file hw3\\wiki\\article_'+str(time)+'.tsv', 'wt',encoding='utf-8') as out_file:\n",
    "                tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "                tsv_writer.writerow(data_tip_merged['Category'])\n",
    "                #aggiungere row con informazioni generiche   \n",
    "                tsv_writer.writerow(data_tip_merged['info'])\n",
    "            time+=1\n",
    "        else:\n",
    "            data_category = ['Title','Intro','Plot']\n",
    "            data_to_merge1 = pd.DataFrame(data_category)\n",
    "            data_tip = [title]\n",
    "            data_to_merge2 = pd.DataFrame(data_tip)\n",
    "            data_tip_merged=pd.merge(data_to_merge1, data_to_merge2, left_index=True, right_index=True) \n",
    "            data_tip_merged.columns = ['Category', 'info']\n",
    "            data_tip_merged=data_tip_merged.dropna(how='all')\n",
    "            data_tip_merged=data_tip_merged.reset_index(drop=True)\n",
    "            with open(r'C:\\Users\\HP\\Desktop\\file hw3\\wiki\\article_'+str(time)+'.tsv', 'wt',encoding='utf-8') as out_file:\n",
    "                \n",
    "                tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "               \n",
    "                tsv_writer.writerow(data_tip_merged['Category'])   \n",
    "                tsv_writer.writerow(data_tip_merged['info'])\n",
    "            time+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a first common step, we preprocess the documents by removing stopwords,removing punctuation and stemming\n",
    "#We do this with a function called stringcleaning()\n",
    "def string_cleaning(text):\n",
    "    if type(text) is str:\n",
    "        \n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        words = tokenizer.tokenize(text)\n",
    "        \n",
    "        stopWords = set(stopwords.words('english'))\n",
    "        wordsFiltered = []\n",
    "        for w in words:\n",
    "            if w not in stopWords:\n",
    "                wordsFiltered.append(w)\n",
    "        \n",
    "        porter = PorterStemmer()\n",
    "        listOfWords = [porter.stem(word) for word in wordsFiltered]\n",
    "        result = ' '.join(listOfWords)\n",
    "    else:\n",
    "        result = np.nan\n",
    "    return result  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjunctive query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a vocabulary in json format that maps each word to an integer (term_id)\n",
    "def building_and_save_vocabulary(n,path):\n",
    "    vocabulary = {}\n",
    "    integer = 1\n",
    "    for i in range(1, n):\n",
    "        try:\n",
    "            df = pd.read_csv(path+str(i)+'.tsv',encoding = 'utf-8', sep=\"\\t\")#Create a dataframe for each tsv\n",
    "            intro_and_plot = df['Intro'][0].split()+df['Plot'][0].split()#Save as a list the Intro and Plot section of each tsv\n",
    "            for word in intro_and_plot:\n",
    "                        if word not in vocabulary:\n",
    "                            vocabulary[word] = integer\n",
    "                            integer += 1 \n",
    "        except:\n",
    "            continue                       \n",
    "    json.dump(vocabulary, open('vocabulary.json', 'w', encoding='utf-8'))                        \n",
    "    json.load(open('vocabulary.json', 'r')) \n",
    "    return vocabulary\n",
    "vocabulary = building_and_save_vocabulary(30001,'C:/Users/simo2/tsv files project aris final and cleaned/articolo')\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create the Inverted index\n",
    "#This function creates an Inverted Index dictionary,so each integer(that refers to a word) has as values the documents in which is contained\n",
    "#We use the dictionary that we have created\n",
    "def invertedIndexAdd(n,vocabulary,path):\n",
    "    Inverted_index={}\n",
    "    for i in range(1,n):\n",
    "        try:\n",
    "            df = pd.read_csv(path+str(i)+'.tsv',encoding = 'utf-8', sep=\"\\t\")\n",
    "            intro_and_plot = df['Intro'][0].split()+df['Plot'][0].split()\n",
    "            if intro_and_plot!=[]:        \n",
    "                for word in intro_and_plot:\n",
    "                    term_id=vocabulary[word]\n",
    "                    if term_id not in Inverted_index:\n",
    "                        s = set()\n",
    "                        a='document_'+str(i)\n",
    "                        s.add(a)\n",
    "                        Inverted_index[term_id]=s\n",
    "                    else:\n",
    "                        s2 = Inverted_index[term_id]\n",
    "                        a = 'document_'+str(i)\n",
    "                        s2.add(a)\n",
    "                        Inverted_index[term_id]=s2\n",
    "        except:\n",
    "            continue               \n",
    "    return Inverted_index\n",
    "Inverted_index=invertedIndexAdd(30001,vocabulary,'C:/Users/simo2/tsv files project aris final and cleaned/articolo')\n",
    "Inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First query execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Wikipedia Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Avengers (2012 film)</td>\n",
       "      <td>\\r\\n\\r\\nMarvel's The Avengers[6] (classified u...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Avengers_(20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thumbelina (1994 film)</td>\n",
       "      <td>Thumbelina (also known as Hans Christian Ander...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Thumbelina_(1994...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Frozen (2013 film)</td>\n",
       "      <td>\\r\\n\\r\\nFrozen is a 2013 American 3D computer-...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Frozen_(2013_film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Chronicles of Narnia: Prince Caspian</td>\n",
       "      <td>\\r\\nThe Chronicles of Narnia: Prince Caspian i...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Chronicles_o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cars 3</td>\n",
       "      <td>\\r\\n\\r\\nCars 3 is a 2017 American 3D computer-...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cars_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Title  \\\n",
       "0                  The Avengers (2012 film)    \n",
       "1                    Thumbelina (1994 film)    \n",
       "2                        Frozen (2013 film)    \n",
       "3  The Chronicles of Narnia: Prince Caspian    \n",
       "4                                    Cars 3    \n",
       "\n",
       "                                               Intro  \\\n",
       "0  \\r\\n\\r\\nMarvel's The Avengers[6] (classified u...   \n",
       "1  Thumbelina (also known as Hans Christian Ander...   \n",
       "2  \\r\\n\\r\\nFrozen is a 2013 American 3D computer-...   \n",
       "3  \\r\\nThe Chronicles of Narnia: Prince Caspian i...   \n",
       "4  \\r\\n\\r\\nCars 3 is a 2017 American 3D computer-...   \n",
       "\n",
       "                                       Wikipedia Url  \n",
       "0  https://en.wikipedia.org/wiki/The_Avengers_(20...  \n",
       "1  https://en.wikipedia.org/wiki/Thumbelina_(1994...  \n",
       "2   https://en.wikipedia.org/wiki/Frozen_(2013_film)  \n",
       "3  https://en.wikipedia.org/wiki/The_Chronicles_o...  \n",
       "4               https://en.wikipedia.org/wiki/Cars_3  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query='Disney movies 2019'\n",
    "cleaned_query=string_cleaning(query).split()#We clean the string\n",
    "cleaned_query = [vocabulary[key] for key in cleaned_query  if key in vocabulary]#Integer from the vocabulary of the query's words \n",
    "#This lines of codes helps us to make a list of the documents that contain all of the query's words\n",
    "documents_appearances = set(Inverted_index[cleaned_query[0]])\n",
    "for i in range(1, len(cleaned_query)):\n",
    "            documents_appearances = documents_appearances.intersection(Inverted_index[cleaned_query[i]])\n",
    "documents_appearances=list(documents_appearances)\n",
    "#We want to create a dataframe to display the output\n",
    "conjunctive_film_dataframe=pd.DataFrame(columns=['Title','Intro','Wikipedia Url'])#We create an empty dataframe\n",
    "for i in range(len(documents_appearances)):#We make a for loop:We retrieve the tsv,we clean it and we append it to the conjunctive film dataframe\n",
    "    number_of_document=(documents_appearances[i].split('_'))[1]\n",
    "    film_dataframe=pd.read_csv(r'C:\\Users\\simo2\\tsv files project aris\\articolo'+number_of_document+'.tsv',encoding = 'utf-8', sep=\"\\t\")\n",
    "    film_dataframe=film_dataframe[['Title','Intro']]\n",
    "    film_dataframe['Wikipedia Url']=links[int(number_of_document)-1]\n",
    "    conjunctive_film_dataframe=conjunctive_film_dataframe.append(film_dataframe,ignore_index=True)\n",
    "conjunctive_film_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function returns a dictonary that maps for each word the list of documents in which it is contained in, and the relative tfIdf score\n",
    "def Index_TFIDF(n,vocabulary,Inverted_index,path):\n",
    "    \n",
    "    inv_ind_TFIDF = defaultdict(list) \n",
    "                    \n",
    "    for i in range(1,n):\n",
    "        try:\n",
    "            df = pd.read_csv(path+str(i)+'.tsv',encoding = 'utf-8', sep=\"\\t\")\n",
    "            \n",
    "            intro_and_plot = df['Intro'][0].split()+df['Plot'][0].split()\n",
    "            dict_TF = defaultdict(int)           \n",
    "            for word in intro_and_plot:\n",
    "                term_id = vocabulary[word]\n",
    "                dict_TF[term_id] += 1\n",
    "                \n",
    "                        \n",
    "                #TFIDF\n",
    "            for term_id in dict_TF:\n",
    "                inv_ind_TFIDF[term_id].append(('document_'+str(i), (dict_TF[term_id]/len(intro_and_plot)) * (np.log(20000/(len(Inverted_index[term_id]))))))\n",
    "        except:\n",
    "            continue\n",
    "    np.save('inv_ind_TFIDF.npy', inv_ind_TFIDF)\n",
    "    inv_ind_TFIDF = np.load('inv_ind_TFIDF.npy',allow_pickle=True).item()\n",
    "    return inv_ind_TFIDF\n",
    "index_tfidf=Index_TFIDF(30001,vocabulary,Inverted_index,'C:/Users/simo2/tsv files project aris final and cleaned/articolo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second query execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose the number of documents you want as output\n",
      "--> 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Wikipedia Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thumbelina (1994 film)</td>\n",
       "      <td>Thumbelina (also known as Hans Christian Ander...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Thumbelina_(1994...</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Avengers (2012 film)</td>\n",
       "      <td>\\r\\n\\r\\nMarvel's The Avengers[6] (classified u...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Avengers_(20...</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cars 3</td>\n",
       "      <td>\\r\\n\\r\\nCars 3 is a 2017 American 3D computer-...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cars_3</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Frozen (2013 film)</td>\n",
       "      <td>\\r\\n\\r\\nFrozen is a 2013 American 3D computer-...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Frozen_(2013_film)</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Chronicles of Narnia: Prince Caspian</td>\n",
       "      <td>\\r\\nThe Chronicles of Narnia: Prince Caspian i...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Chronicles_o...</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Title  \\\n",
       "0                    Thumbelina (1994 film)    \n",
       "1                  The Avengers (2012 film)    \n",
       "2                                    Cars 3    \n",
       "3                        Frozen (2013 film)    \n",
       "4  The Chronicles of Narnia: Prince Caspian    \n",
       "\n",
       "                                               Intro  \\\n",
       "0  Thumbelina (also known as Hans Christian Ander...   \n",
       "1  \\r\\n\\r\\nMarvel's The Avengers[6] (classified u...   \n",
       "2  \\r\\n\\r\\nCars 3 is a 2017 American 3D computer-...   \n",
       "3  \\r\\n\\r\\nFrozen is a 2013 American 3D computer-...   \n",
       "4  \\r\\nThe Chronicles of Narnia: Prince Caspian i...   \n",
       "\n",
       "                                       Wikipedia Url  Similarity  \n",
       "0  https://en.wikipedia.org/wiki/Thumbelina_(1994...        0.91  \n",
       "1  https://en.wikipedia.org/wiki/The_Avengers_(20...        0.91  \n",
       "2               https://en.wikipedia.org/wiki/Cars_3        0.87  \n",
       "3   https://en.wikipedia.org/wiki/Frozen_(2013_film)        0.81  \n",
       "4  https://en.wikipedia.org/wiki/The_Chronicles_o...        0.77  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = int(input('Choose the number of documents you want as output\\n--> '))#The user can choices how much documents he wants to display in the output\n",
    "cleaned_query=string_cleaning(query).split()\n",
    "#We create a TF dictionary of all the words in the query\n",
    "dict_TF = defaultdict(float)\n",
    "len_q = len(cleaned_query)\n",
    "for word in cleaned_query:\n",
    "    term_id = vocabulary[word]\n",
    "    dict_TF[term_id] += 1/len_q\n",
    "\n",
    "int_query = [vocabulary[key] for key in cleaned_query  if key in vocabulary]#The same as we did in the first search engine\n",
    "cleaned_query = dict_TF #Our cleaned_query becomes the dict that we have just created\n",
    "documents_appearances = set(Inverted_index[int_query[0]])#We do the same as we did in the first search engine\n",
    "for i in range(1, len(int_query)):\n",
    "            documents_appearances = documents_appearances.intersection(Inverted_index[int_query[i]])\n",
    "documents_appearances=list(documents_appearances)\n",
    "result = documents_appearances \n",
    "dict_list_doc_words = defaultdict(list)\n",
    "for word in cleaned_query:#The tfidf of each word related to the document\n",
    "        for elem in index_tfidf[word]:\n",
    "\n",
    "            if elem[0] in result:\n",
    "                dict_list_doc_words[elem[0]].append(elem[1]) \n",
    "heap = []#We create an heap structure to display only the first k documents as output\n",
    "list_query = list(cleaned_query.values())\n",
    "def cosine_similarity(array1,array2):#cosine similarity function\n",
    "    return(1 - spatial.distance.cosine(array1, array2))\n",
    "for doc in dict_list_doc_words:\n",
    "    heapq.heappush(heap, (cosine_similarity(list_query, dict_list_doc_words[doc]), doc))\n",
    "heap_result = heapq.nlargest(k, heap)#We have a list of tuples.Each tuple is the document and the similarity of this document with the query\n",
    "similarity_film_dataframe=pd.DataFrame(columns=['Title','Intro','Wikipedia Url','Similarity'])\n",
    "for i in range(len(heap_result)):\n",
    "    number_of_document=(heap_result[i][1].split('_'))[1]\n",
    "    film_dataframe=pd.read_csv(r'C:\\Users\\simo2\\tsv files project aris\\articolo'+number_of_document+'.tsv',encoding = 'utf-8', sep=\"\\t\")\n",
    "    film_dataframe=film_dataframe[['Title','Intro']]\n",
    "    film_dataframe['Wikipedia Url']=links[int(number_of_document)-1]\n",
    "    film_dataframe['Similarity']=round(heap_result[i][0],2)\n",
    "    similarity_film_dataframe=similarity_film_dataframe.append(film_dataframe,ignore_index=True) \n",
    "similarity_film_dataframe   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
